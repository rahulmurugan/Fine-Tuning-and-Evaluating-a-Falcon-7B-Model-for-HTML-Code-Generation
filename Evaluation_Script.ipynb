{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDoT0cYWDWg3"
      },
      "outputs": [],
      "source": [
        "#Saving the Fine-tuned model\n",
        "from transformers import AutoModelForCausalLM\n",
        "output_dir = \"/content/output_dir\"\n",
        "\n",
        "\n",
        "final_model = AutoModelForCausalLM.from_pretrained(\n",
        "output_dir, local_files_only=True,\n",
        "quantization_config=bnb_config,\n",
        "trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation using Loss function\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the inference function\n",
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "    input_ids = tokenizer.encode(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_tokens\n",
        "    )\n",
        "\n",
        "    # Ensure that pad_token_id is set for open-end generation\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    # Generate sequences while explicitly setting attention_mask\n",
        "    generated_tokens_with_prompt = model.generate(\n",
        "        input_ids=input_ids.to(model.device),\n",
        "        max_length=max_output_tokens,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        attention_mask=input_ids.to(model.device)  # Setting attention mask\n",
        "    )\n",
        "\n",
        "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "    # Tokenize the generated text\n",
        "    generated_tokens_answer = tokenizer.encode(generated_text_answer, return_tensors=\"pt\").squeeze()\n",
        "\n",
        "    return generated_tokens_answer\n",
        "\n",
        "# Define a function to calculate the loss between predicted and target outputs\n",
        "def calculate_loss(predicted_tokens, target_tokens):\n",
        "    # Calculate the loss using a suitable loss function (e.g., CrossEntropyLoss for token-level comparison)\n",
        "    loss_function = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "    loss = loss_function(predicted_tokens, target_tokens)\n",
        "\n",
        "    return loss.item()  # Return the loss value as a scalar\n",
        "\n",
        "\n",
        "evaluation_dataset = split_dataset[\"test\"]\n",
        "\n",
        "# Define an empty list to store loss values\n",
        "losses = []\n",
        "num_samples_to_process = 10\n",
        "# Evaluate the loss for each item in the test dataset\n",
        "for i, item in tqdm(enumerate(evaluation_dataset[:num_samples_to_process])):\n",
        "    print(\"i Evaluating: \" + str(item))\n",
        "    question = item['instruction']\n",
        "    answer = item['output']\n",
        "\n",
        "    try:\n",
        "        predicted_tokens = inference(question, final_model, tokenizer)\n",
        "        target_tokens = tokenizer.encode(answer, return_tensors=\"pt\").squeeze()\n",
        "        loss_value = calculate_loss(predicted_tokens, target_tokens)\n",
        "        losses.append(loss_value)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Calculate the average loss across all predictions\n",
        "average_loss = sum(losses) / len(losses) if losses else 0\n",
        "print(f\"Average Loss: {average_loss}\")\n"
      ],
      "metadata": {
        "id": "LlqYLAHiD4Fz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation by Comparision\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define a function to check exact match between answers\n",
        "def is_exact_match(a, b):\n",
        "    return a.strip() == b.strip()\n",
        "\n",
        "# Define a function for inference\n",
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    input_ids = tokenizer.encode(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_tokens\n",
        "    )\n",
        "\n",
        "    generated_tokens_with_prompt = model.generate(\n",
        "        input_ids=input_ids.to(model.device),\n",
        "        max_length=max_output_tokens\n",
        "    )\n",
        "\n",
        "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "    return generated_text_answer\n",
        "\n",
        "# Load the evaluation dataset\n",
        "\n",
        "evaluation_dataset = split_dataset[\"test\"]\n",
        "\n",
        "# Modify the evaluation loop for your split test dataset\n",
        "metrics = {'exact_matches': []}\n",
        "predictions = []\n",
        "\n",
        "for i, item in tqdm(enumerate(evaluation_dataset)):\n",
        "    question = item['instruction']\n",
        "    answer = item['output']\n",
        "\n",
        "    try:\n",
        "        predicted_answer = inference(question, final_model, tokenizer)  # Use your trained peft_model and tokenizer\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    predictions.append([predicted_answer, answer])\n",
        "    exact_match = is_exact_match(predicted_answer, answer)\n",
        "    metrics['exact_matches'].append(exact_match)\n",
        "\n",
        "print('Number of exact matches: ', sum(metrics['exact_matches']))\n",
        "df = pd.DataFrame(predictions, columns=[\"predicted_output\", \"target_output\"])\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "7hH3bbOgD-dK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}