{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ADcvW5SiCinU"
      },
      "outputs": [],
      "source": [
        "#Using PEFT\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Prepare the model for K-BiT training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Function to print trainable parameters in the model\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "# Prepare the model for K-BiT training again (this seems redundant)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Define the LoRA configuration\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_rank = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_rank,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Obtain the PEFT model using the provided configuration\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "# Set tokenizer pad_token to eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=4,\n",
        "    logging_steps=25,\n",
        "    output_dir=\"output_dir\",\n",
        "    save_strategy='epoch',\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_ratio=0.05,\n",
        ")\n",
        "\n",
        "# Initialize Trainer for training\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=split_dataset[\"train\"],\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zi7BpPmiCu_t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}